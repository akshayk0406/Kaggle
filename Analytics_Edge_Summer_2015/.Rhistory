KMC = kmeans(healthyVector,centers=num_cluster,iter.max=1000)
str(KMC)
healthyClusters = KMC$cluster
healthyClusters
str(KMC)
dim(healthyClusters) = c(nrow(healthyMatrix),ncol(healthyMatrix))
image(healthyClusters,axes=FALSE,col=rainbow(num_cluster))
download.file("https://courses.edx.org/asset-v1:MITx+15.071x_2a+2T2015+type@asset+block/tumor.csv",destfile="data/tumor.csv",method="curl")
tumor = read.csv("data/tumor.csv",header=FALSE)
tumorMatrix = as.matrix(tumor)
tumorVector = as.vector(tumorMatrix)
install.packages("flexclust")
library(flexclust)
KMC.kcca = as.kcca(KMC,healthyVector)
tumorClusters = predict(KMC.kcca,tumorVector)
tumorClusters
dim(tumorClusters) = c(nrow(tumorMatrix),ncol(tumorMatrix))
tumorClusters = predict(KMC.kcca,tumorVector)
dim(tumorClusters) = c(nrow(tumorMatrix),ncol(tumorMatrix))
image(tumorClusters,axes=FALSE,col=rainbow(num_cluster))
image(tumorClusters,axes=FALSE,col=rainbow(num_cluster))
library(alr4)
m1 <- lm(log(V) ~ log(C),data=haystacks)
haystacks <- read.table(
"http://www.stat.umn.edu/~sandy/courses/5302/data/haystacks/haystack.txt",
header=TRUE)
m1 <- lm(log(V) ~ log(C),data=haystacks)
str(haystacks)
m1 <- lm(log(Vol) ~ log(C),data=haystacks)
summary(m1)
xbar <- mean(log(haystacks$C))
SXX <- sum((log(haystacks$Vol) - xbar)^2)
SXX
xbar
0.2022/1685.846
SXX <- sum((log(haystacks$C) - xbar)^2)
SXX
xbar
0.2022/0.6454
0.2571*0.6454617
0.2022/118
m1 <- lm(log(Vol) ~ log(C),data=haystacks)
m1$residuals
sum((m1$residuals)^2)
sum((m1$residuals)^2)/118
sqrt(sum((m1$residuals)^2)/118)
sum((m1$residuals)^2)
sum((m1$residuals)^2)/118
x=sum((m1$residuals)^2)/118
x/0.64
x=sum((m1$residuals)^2)/128
x=sum((m1$residuals)^2)/120
x/0.64
x
sum((m1$residuals)^2)
sum((m1$residuals)^2)
0.2022/sqrt(0.6454)
MySubmission = data.frame(UniqueID = eBayTest$UniqueID, Probability1 = PredTest)
# KAGGLE COMPETITION - GETTING STARTED
# This script file is intended to help you get started on the Kaggle platform, and to show you how to make a submission to the competition.
# Let's start by reading the data into R
# Make sure you have downloaded these files from the Kaggle website, and have navigated to the directory where you saved the files on your computer
# We are adding in the argument stringsAsFactors=FALSE, since we have some text fields
eBayTrain = read.csv("eBayiPadTrain.csv", stringsAsFactors=FALSE)
eBayTest = read.csv("eBayiPadTest.csv", stringsAsFactors=FALSE)
# We will just create a simple logistic regression model, to predict Sold using Price:
SimpleMod = glm(sold ~ startprice, data=eBayTrain, family=binomial)
# And then make predictions on the test set:
PredTest = predict(SimpleMod, newdata=eBayTest, type="response")
# We can't compute the accuracy or AUC on the test set ourselves, since we don't have the dependent variable on the test set (you can compute it on the training set though!).
# However, you can submit the file on Kaggle to see how well the model performs. You can make up to 5 submissions per day, so don't hesitate to just upload a solution to see how you did.
# Let's prepare a submission file for Kaggle (for more about this, see the "Evaluation" page on the competition site):
MySubmission = data.frame(UniqueID = eBayTest$UniqueID, Probability1 = PredTest)
write.csv(MySubmission, "SubmissionSimpleLog.csv", row.names=FALSE)
# You should upload the submission "SubmissionSimpleLog.csv" on the Kaggle website to use this as a submission to the competition
# This model was just designed to help you get started - to do well in the competition, you will need to build better models!
setwd("/Users/akshaykulkarni/Documents/Kaggle/Analytics_Edge_Summer_2015")
# KAGGLE COMPETITION - GETTING STARTED
# This script file is intended to help you get started on the Kaggle platform, and to show you how to make a submission to the competition.
# Let's start by reading the data into R
# Make sure you have downloaded these files from the Kaggle website, and have navigated to the directory where you saved the files on your computer
# We are adding in the argument stringsAsFactors=FALSE, since we have some text fields
eBayTrain = read.csv("eBayiPadTrain.csv", stringsAsFactors=FALSE)
eBayTest = read.csv("eBayiPadTest.csv", stringsAsFactors=FALSE)
# We will just create a simple logistic regression model, to predict Sold using Price:
SimpleMod = glm(sold ~ startprice, data=eBayTrain, family=binomial)
# And then make predictions on the test set:
PredTest = predict(SimpleMod, newdata=eBayTest, type="response")
# We can't compute the accuracy or AUC on the test set ourselves, since we don't have the dependent variable on the test set (you can compute it on the training set though!).
# However, you can submit the file on Kaggle to see how well the model performs. You can make up to 5 submissions per day, so don't hesitate to just upload a solution to see how you did.
# Let's prepare a submission file for Kaggle (for more about this, see the "Evaluation" page on the competition site):
MySubmission = data.frame(UniqueID = eBayTest$UniqueID, Probability1 = PredTest)
write.csv(MySubmission, "SubmissionSimpleLog.csv", row.names=FALSE)
# You should upload the submission "SubmissionSimpleLog.csv" on the Kaggle website to use this as a submission to the competition
# This model was just designed to help you get started - to do well in the competition, you will need to build better models!
str(eBayTrain)
head(eBayTrain)
setwd("/Users/akshaykulkarni/RProjects/analyticsedge")
download.file("https://courses.edx.org/asset-v1:MITx+15.071x_2a+2T2015+type@asset+block/WHO.csv",destfile="data/WHO.csv",method="curl")
who = read.csv("data/WHO.csv")
str(who)
plot(who$GNI ~ who$FertilityRate)
install.packages("ggplot2")
library(ggplot2)
scplot = ggplot(who,aes(x=GNI,y=FertilityRate))
plot(who$FertilityRate ~ who$GNI)
scplot = ggplot(who,aes(x=GNI,y=FertilityRate))
scplot + geom_point()
scplot = ggplot(who,aes(x=GNI,y=FertilityRate))
scplot + geom_point(color="blue",size=3,shape=17)
scplot = ggplot(who,aes(x=GNI,y=FertilityRate))
scplot + geom_point(color="darkred",size=3,shape=8)
scplot = ggplot(who,aes(x=GNI,y=FertilityRate))
scplot + geom_point(color="darkred",size=3,shape=8) + ggtitle("Fertility Rate vs Gross National Income")
scplot = ggplot(who,aes(x=GNI,y=FertilityRate))
fertility_gni_plot = scplot + geom_point(color="darkred",size=3,shape=8) + ggtitle("Fertility Rate vs Gross National Income")
scplot = ggplot(who,aes(x=GNI,y=FertilityRate))
fertility_gni_plot = scplot + geom_point(color="darkred",size=3,shape=8) + ggtitle("Fertility Rate vs Gross National Income")
pdf("data/fertility_gni_plot.pdf")
print(fertility_gni_plot)
dev.off()
fertility_gni_plot = scplot + geom_point(color="darkred",size=3,shape=15) + ggtitle("Fertility Rate vs Gross National Income")
fertility_gni_plot = scplot + geom_point(color="darkred",size=3,shape=15) + ggtitle("Fertility Rate vs Gross National Income")
pdf("data/fertility_gni_plot.pdf")
print(fertility_gni_plot)
dev.off()
ggplot(who,aes(x=GNI,y=FertilityRate,color=Region)) + geom_point()
ggplot(who,aes(x=GNI,y=FertilityRate,color=LifeExpectancy)) + geom_point()
ggplot(who,aes=(x=FertilityRate,y=Under15)) + geom_point()
ggplot(who,aes(x=FertilityRate,y=Under15)) + geom_point()
ggplot(who,aes(x=log(FertilityRate),y=Under15)) + geom_point()
model = lm(Under15 ~ log(FertilityRate),data=who)
summary(model)
ggplot(who,aes(x=log(FertilityRate),y=Under15)) + geom_point() + stat_method(method="lm")
ggplot(who,aes(x=log(FertilityRate),y=Under15)) + geom_point() + stat_smooth(method="lm")
ggplot(who,aes(x=log(FertilityRate),y=Under15)) + geom_point() + stat_smooth(method="lm",level=0.99)
ggplot(who,aes(x=log(FertilityRate),y=Under15)) + geom_point() + stat_smooth(method="lm",se=FALSE)
ggplot(WHO, aes(x = FertilityRate, y = Under15)) + geom_point()
ggplot(who, aes(x = FertilityRate, y = Under15)) + geom_point()
ggplot(who, aes(x = FertilityRate, y = Under15,color=Region)) + geom_point()
#Predictive Policing in Chicago Area
download.file("https://courses.edx.org/asset-v1:MITx+15.071x_2a+2T2015+type@asset+block/mvt.csv",destfile="data/mvt.csv",method="curl")
mvt = read.csv("data/mvt.csv")
str(mvt)
mvt = read.csv("data/mvt.csv",stringsAsFactors=FALSE)
str(mvt)
mvt$Date = strptime(mvt$Date,format="%m/%d/%y %H:%M")
head(mvt)
mvt$WeekDay = weekdays(mvt$Date)
mvt$Hour = mvt$Date$hour
head(mvt)
table(mvt$WeekDay)
week_day_crime = as.data.frame(table(mvt$WeekDay))
str(week_day_crime)
ggplot(week_day_crime,aes(x=Var1,y=Freq)) + geom_line(aes(group=1))
week_day_crime$Var1 = factor(week_day_crime$Var1,ordered=TRUE,levels=c("Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"))
ggplot(week_day_crime,aes(x=Var1,y=Freq)) + geom_line(aes(group=1))
week_day_crime$Var1 = factor(week_day_crime$Var1,ordered=TRUE,levels=c("Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday"))
ggplot(week_day_crime,aes(x=Var1,y=Freq)) + geom_line(aes(group=1)) + xlab("Day of the week") + ylab("Total Motor Vechicle Theft")
ggplot(week_day_crime,aes(x=Var1,y=Freq)) + geom_line(aes(group=1), linetype=2) + xlab("Day of the week") + ylab("Total Motor Vechicle Theft")
ggplot(week_day_crime,aes(x=Var1,y=Freq)) + geom_line(aes(group=1), alpha=0.3) + xlab("Day of the week") + ylab("Total Motor Vechicle Theft")
ggplot(week_day_crime,aes(x=Var1,y=Freq)) + geom_line(aes(group=1), alpha=0.3) + xlab("Day of the week") + ylab("Total Motor Vechicle Theft")
ggplot(week_day_crime,aes(x=Var1,y=Freq)) + geom_line(aes(group=1)) + xlab("Day of the week") + ylab("Total Motor Vechicle Theft")
table(mvt$Weekday,mvt$Hour)
str(mvt)
table(mvt$Weekday,mvt$Hour)
table(mvt$Weekday,mvt$Hour)
nrow(mvt$Weekday)
mvt$nrow
mvt$Weekday
table(mvt$WeekDay,mvt$Hour)
DayHourCounts = as.data.frame(table(mvt$WeekDay,mvt$Hour))
str(DayHourCounts)
DayHourCounts$Hour = as.numeric(as.character(DayHourCounts$Var2))
str(DayHourCounts)
ggplot(DayHourCounts,aes(x=Hour,y=Freq))+geom_line(aes(group=1))
ggplot(DayHourCounts,aes(x=Hour,y=Freq))+geom_line(aes(group=Var1,color=Var1))
ggplot(DayHourCounts,aes(x=Hour,y=Freq))+geom_line(aes(group=Var1,color=Var1),size=2)
DayHourCounts$Var1 = factor(DayHourCounts$Var1,ordered=TRUE,levels=c("Monday","Tuesday","Wednesday","Thursday","Friday","Saturday","Sunday"))
ggplot(DayHourCounts,aes(x=Hour,y=Var1))+geom_tile(aes(fill=Freq))
Factor variable
ggplot(DayHourCounts,aes(x=Hour,y=Var1))+geom_tile(aes(fill=Freq)) + scale_fill_gradient(name="Total MV Thefts") + theme(axis.title.y=element_blank())
ggplot(DayHourCounts,aes(x=Hour,y=Var1))+geom_tile(aes(fill=Freq))
+ scale_fill_gradient(name="Total MV Thefts",low="white",high="red") +
theme(axis.title.y=element_blank()) #For heat-map sue geom_tile
ggplot(DayHourCounts,aes(x=Hour,y=Var1))
+ geom_tile(aes(fill=Freq))
+ scale_fill_gradient(name="Total MV Thefts",low="white",high="red")
+ theme(axis.title.y=element_blank())
ggplot(DayHourCounts,aes(x=Hour,y=Var1)) + geom_tile(aes(fill=Freq))  + scale_fill_gradient(name="Total MV Thefts",low="white",high="red")  + theme(axis.title.y=element_blank()) #For heat-map sue geom_tile
install.packages("maps")
install.packages("ggmap")
ggmap(chicago)
install.packages("ggmap")
chicago = get_map(location="chicage",zoom=11)
ggmap(chicago)
library(ggmap)
library(maps)
chicago = get_map(location="chicage",zoom=11)
ggmap(chicago)
str(mvt)
ggmap(chicago) + geom_point(data=mvt[1:100,],aes(x=Longitude,y=Latitude))
LatLonCounts = as.data.frame(table(round(mvt$Longitude,2),round(mvt$Latitude,2)))
str(LatLonCounts)
LatLonCounts$Lon = as.numeric(as.character(LatLonCounts$Var1))
LatLonCounts$Lat = as.numeric(as.character(LatLonCounts$Var2))
str(LatLonCounts)
ggmap(chicago) + geom_point(data=LatLonCounts,aes(x=Lon,y=Lat,color=Freq,size=Freq))
ggmap(chicago) + geom_point(data=LatLonCounts,aes(x=Lon,y=Lat,color=Freq,size=Freq)) + scale_color_gradient(low="yellow",high="red")
str(mvt)
week_day_crime
haystacks
m1
tval = -0.487
pval = 2*pt(abs(tval),m1$df,lower.tail=FALSE)
pval
t=qt(.975,m1$df)
t
?qt
fit.70 <- coef(m1)[1] + coef(m1)[2]*log(70)
vcov(m1)
se.fit.70 <- sqrt(vcov(m1)[1, 1] + log(70)^2 * vcov(m1)[2, 2] + 2 * log(70) * vcov(m1)[1, 2]
)
data.frame(fitted.value=fit.70, se.fit=se.fit.70)
t.value <- qt(.975, m1$df.residual)
c(-1, 1) * t.value * se.fit.70)
c(-1, 1) * t.value * se.fit.70
predict(m1,data.frame(C=c(60,70,80)),interval="confidence")
setwd("/Users/akshaykulkarni/Documents/Kaggle/Analytics_Edge_Summer_2015")
eBayTrain = read.csv("eBayiPadTrain.csv", stringsAsFactors=FALSE)
eBayTest = read.csv("eBayiPadTest.csv", stringsAsFactors=FALSE)
MySubmission = data.frame(UniqueID = eBayTest$UniqueID, Probability1 = PredTest)
write.csv(MySubmission, "SubmissionSimpleLog.csv", row.names=FALSE)
str(eBayTrain)
unique(eBayTrain$condition)
unique(eBayTrain$color)
unique(eBayTrain$carrier)
unique(eBayTrain$productline)
# KAGGLE COMPETITION - GETTING STARTED
# This script file is intended to help you get started on the Kaggle platform, and to show you how to make a submission to the competition.
# Let's start by reading the data into R
# Make sure you have downloaded these files from the Kaggle website, and have navigated to the directory where you saved the files on your computer
# We are adding in the argument stringsAsFactors=FALSE, since we have some text fields
eBayTrain = read.csv("eBayiPadTrain.csv", stringsAsFactors=FALSE)
eBayTest = read.csv("eBayiPadTest.csv", stringsAsFactors=FALSE)
# We will just create a simple logistic regression model, to predict Sold using Price:
SimpleMod = glm(sold ~ startprice, data=eBayTrain, family=binomial)
# And then make predictions on the test set:
PredTest = predict(SimpleMod, newdata=eBayTest, type="response")
# We can't compute the accuracy or AUC on the test set ourselves, since we don't have the dependent variable on the test set (you can compute it on the training set though!).
# However, you can submit the file on Kaggle to see how well the model performs. You can make up to 5 submissions per day, so don't hesitate to just upload a solution to see how you did.
# Let's prepare a submission file for Kaggle (for more about this, see the "Evaluation" page on the competition site):
MySubmission = data.frame(UniqueID = eBayTest$UniqueID, Probability1 = PredTest)
write.csv(MySubmission, "SubmissionSimpleLog.csv", row.names=FALSE)
# You should upload the submission "SubmissionSimpleLog.csv" on the Kaggle website to use this as a submission to the competition
# This model was just designed to help you get started - to do well in the competition, you will need to build better models!
summary(SimpleMod)
# KAGGLE COMPETITION - GETTING STARTED
# This script file is intended to help you get started on the Kaggle platform, and to show you how to make a submission to the competition.
# Let's start by reading the data into R
# Make sure you have downloaded these files from the Kaggle website, and have navigated to the directory where you saved the files on your computer
# We are adding in the argument stringsAsFactors=FALSE, since we have some text fields
eBayTrain = read.csv("eBayiPadTrain.csv", stringsAsFactors=FALSE)
eBayTest = read.csv("eBayiPadTest.csv", stringsAsFactors=FALSE)
# We will just create a simple logistic regression model, to predict Sold using Price:
SimpleMod = glm(sold ~ ., data=eBayTrain, family=binomial)
# And then make predictions on the test set:
PredTest = predict(SimpleMod, newdata=eBayTest, type="response")
# We can't compute the accuracy or AUC on the test set ourselves, since we don't have the dependent variable on the test set (you can compute it on the training set though!).
# However, you can submit the file on Kaggle to see how well the model performs. You can make up to 5 submissions per day, so don't hesitate to just upload a solution to see how you did.
# Let's prepare a submission file for Kaggle (for more about this, see the "Evaluation" page on the competition site):
MySubmission = data.frame(UniqueID = eBayTest$UniqueID, Probability1 = PredTest)
write.csv(MySubmission, "SubmissionSimpleLog.csv", row.names=FALSE)
# You should upload the submission "SubmissionSimpleLog.csv" on the Kaggle website to use this as a submission to the competition
# This model was just designed to help you get started - to do well in the competition, you will need to build better models!
summary(SimpleMod)
str(eBayTrain)
setwd("/Users/akshaykulkarni/Documents/Kaggle/Analytics_Edge_Summer_2015")
eBayTrain = read.csv("eBayiPadTrain.csv", stringsAsFactors=FALSE)
eBayTest = read.csv("eBayiPadTest.csv", stringsAsFactors=FALSE)
MySubmission = data.frame(UniqueID = eBayTest$UniqueID, Probability1 = PredTest)
write.csv(MySubmission, "SubmissionSimpleLog.csv", row.names=FALSE)
setwd("/Users/akshaykulkarni/Documents/Kaggle/Analytics_Edge_Summer_2015")
library(tm)
library(SnowballC)
eBayTrain = read.csv("eBayiPadTrain.csv", stringsAsFactors=FALSE)
eBayTest = read.csv("eBayiPadTest.csv", stringsAsFactors=FALSE)
prod_description = eBayTrain$description
corpus = Corpus(VectorSource(prod_description))
corpus = tm_map(corpus,lower)
corpus = tm_map(corpus,PlainTextDocument)
corpus = tm_map(corpus,removePunctuation)
corpus = tm_map(corpus,removeWords)
corpus = tm_map(corpus,StemDocument)
frequencies = DocumentTermMatrix(corpus)
corpus[[1]]
setwd("/Users/akshaykulkarni/Documents/Kaggle/Analytics_Edge_Summer_2015")
library(tm)
library(SnowballC)
eBayTrain = read.csv("eBayiPadTrain.csv", stringsAsFactors=FALSE)
eBayTest = read.csv("eBayiPadTest.csv", stringsAsFactors=FALSE)
prod_description = eBayTrain$description
corpus = Corpus(VectorSource(prod_description))
corpus = tm_map(corpus,lower)
corpus = tm_map(corpus,PlainTextDocument)
corpus = tm_map(corpus,removePunctuation)
corpus = tm_map(corpus,removeWords)
corpus = tm_map(corpus,stemDocument)
frequencies = DocumentTermMatrix(corpus)
corpus[[1]]
prod_description = eBayTrain$description
corpus = Corpus(VectorSource(prod_description))
corpus = tm_map(corpus,lower)
corpus = tm_map(corpus,PlainTextDocument)
corpus = tm_map(corpus,removePunctuation)
corpus[[1]]
corpus = tm_map(corpus,removeWords)
corpus[[1]]
corpus = tm_map(corpus,removeWords)
corpus[[1]]
prod_description = eBayTrain$description
corpus = Corpus(VectorSource(prod_description))
corpus = tm_map(corpus,lower,lazy=TRUE)
corpus = tm_map(corpus,PlainTextDocument)
prod_description = eBayTrain$description
corpus = Corpus(VectorSource(prod_description))
corpus = tm_map(corpus,lower,lazy=TRUE)
corpus = tm_map(corpus,PlainTextDocument,lazy=TRUE)
corpus = tm_map(corpus,removePunctuation,lazy=TRUE)
prod_description = eBayTrain$description
corpus = Corpus(VectorSource(prod_description))
corpus = tm_map(corpus,lower,lazy=TRUE)
corpus = tm_map(corpus,PlainTextDocument,lazy=TRUE)
corpus = tm_map(corpus,removePunctuation,lazy=TRUE)
corpus = tm_map(corpus,removeWords,lazy=TRUE)
corpus = tm_map(corpus,stemDocument,lazy=TRUE)
frequencies = DocumentTermMatrix(corpus)
corpus[[1]]
prod_description = eBayTrain$description
corpus = Corpus(VectorSource(prod_description))
corpus = tm_map(corpus,lower,lazy=TRUE)
corpus = tm_map(corpus,PlainTextDocument,lazy=TRUE)
corpus = tm_map(corpus,removePunctuation,lazy=TRUE)
corpus = tm_map(corpus,removeWords,lazy=TRUE)
corpus = tm_map(corpus,stemDocument,lazy=TRUE)
corpus[[1]]
corpus[[1]]
prod_description = eBayTrain$description
corpus = Corpus(VectorSource(prod_description))
corpus = tm_map(corpus,lower,lazy=TRUE)
corpus[[1]]
prod_description = eBayTrain$description
corpus = Corpus(VectorSource(prod_description))
corpus = tm_map(corpus,tolower,lazy=TRUE)
corpus = tm_map(corpus,PlainTextDocument,lazy=TRUE)
corpus = tm_map(corpus,removePunctuation,lazy=TRUE)
corpus = tm_map(corpus,removeWords,lazy=TRUE)
corpus = tm_map(corpus,stemDocument,lazy=TRUE)
frequencies = DocumentTermMatrix(corpus)
prod_description = eBayTrain$description
corpus = Corpus(VectorSource(prod_description))
corpus = tm_map(corpus,tolower,lazy=TRUE)
corpus = tm_map(corpus,PlainTextDocument,lazy=TRUE)
corpus = tm_map(corpus,removePunctuation,lazy=TRUE)
corpus = tm_map(corpus,removeWords,lazy=TRUE)
corpus = tm_map(corpus,stemDocument,lazy=TRUE)
corpus[[1]]
prod_description = eBayTrain$description
corpus = Corpus(VectorSource(prod_description))
corpus = tm_map(corpus,tolower,lazy=TRUE)
corpus = tm_map(corpus,PlainTextDocument,lazy=TRUE)
corpus = tm_map(corpus,removePunctuation,lazy=TRUE)
corpus = tm_map(corpus,removeWords,lazy=TRUE)
corpus[[1]]
prod_description = eBayTrain$description
corpus = Corpus(VectorSource(prod_description))
corpus[[1]]
corpus = tm_map(corpus,tolower,lazy=TRUE)
corpus[[1]]
corpus = tm_map(corpus,PlainTextDocument,lazy=TRUE)
corpus[[1]]
corpus = tm_map(corpus,removePunctuation,lazy=TRUE)
corpus[[1]]
corpus = tm_map(corpus,removeWords,lazy=TRUE)
corpus[[1]]
corpus = tm_map(corpus,stemDocument,lazy=TRUE)
corpus[[1]]
frequencies = DocumentTermMatrix(corpus)
prod_description = eBayTrain$description
corpus = Corpus(VectorSource(prod_description))
corpus = tm_map(corpus,content_transformer(tolower),lazy=TRUE)
corpus = tm_map(corpus,PlainTextDocument,lazy=TRUE)
corpus = tm_map(corpus,removePunctuation,lazy=TRUE)
corpus = tm_map(corpus,removeWords,lazy=TRUE)
corpus = tm_map(corpus,stemDocument,lazy=TRUE)
frequencies = DocumentTermMatrix(corpus)
setwd("/Users/akshaykulkarni/Documents/Kaggle/Analytics_Edge_Summer_2015")
library(tm)
library(SnowballC)
eBayTrain = read.csv("eBayiPadTrain.csv", stringsAsFactors=FALSE)
eBayTest = read.csv("eBayiPadTest.csv", stringsAsFactors=FALSE)
prod_description = eBayTrain$description
corpus = Corpus(VectorSource(prod_description))
corpus = tm_map(corpus,tolower,lazy=TRUE)
corpus = tm_map(corpus,PlainTextDocument,lazy=TRUE)
corpus = tm_map(corpus,removePunctuation,lazy=TRUE)
corpus = tm_map(corpus,removeWords,lazy=TRUE)
corpus = tm_map(corpus,stemDocument,lazy=TRUE)
frequencies = DocumentTermMatrix(corpus)
setwd("/Users/akshaykulkarni/Documents/Kaggle/Analytics_Edge_Summer_2015")
library(tm)
library(SnowballC)
eBayTrain = read.csv("eBayiPadTrain.csv", stringsAsFactors=FALSE)
eBayTest = read.csv("eBayiPadTest.csv", stringsAsFactors=FALSE)
prod_description = eBayTrain$description
corpus = Corpus(VectorSource(prod_description))
corpus = tm_map(corpus,tolower)
corpus = tm_map(corpus,PlainTextDocument)
corpus = tm_map(corpus,removePunctuation)
corpus = tm_map(corpus,removeWords)
corpus = tm_map(corpus,stemDocument)
frequencies = DocumentTermMatrix(corpus)
getOption("mc.cores", 2L)
setwd("/Users/akshaykulkarni/Documents/Kaggle/Analytics_Edge_Summer_2015")
library(tm)
library(SnowballC)
eBayTrain = read.csv("eBayiPadTrain.csv", stringsAsFactors=FALSE)
eBayTest = read.csv("eBayiPadTest.csv", stringsAsFactors=FALSE)
prod_description = eBayTrain$description
corpus = Corpus(VectorSource(prod_description))
corpus = tm_map(corpus,tolower,mc.cores=1)
setwd("/Users/akshaykulkarni/Documents/Kaggle/Analytics_Edge_Summer_2015")
library(tm)
library(SnowballC)
eBayTrain = read.csv("eBayiPadTrain.csv", stringsAsFactors=FALSE)
eBayTest = read.csv("eBayiPadTest.csv", stringsAsFactors=FALSE)
prod_description = eBayTrain$description
corpus = Corpus(VectorSource(prod_description))
corpus = tm_map(corpus,tolower)
corpus = tm_map(corpus,PlainTextDocument)
corpus = tm_map(corpus,removePunctuation)
corpus = tm_map(corpus,removeWords)
corpus = tm_map(corpus,stemDocument)
frequencies = DocumentTermMatrix(corpus)
setwd("/Users/akshaykulkarni/Documents/Kaggle/Analytics_Edge_Summer_2015")
library(tm)
library(SnowballC)
eBayTrain = read.csv("eBayiPadTrain.csv", stringsAsFactors=FALSE)
eBayTest = read.csv("eBayiPadTest.csv", stringsAsFactors=FALSE)
prod_description = eBayTrain$description
corpus = Corpus(VectorSource(prod_description))
corpus = tm_map(corpus,tolower,lazy=TRUE)
corpus = tm_map(corpus,PlainTextDocument,lazy=TRUE)
corpus = tm_map(corpus,removePunctuation,lazy=TRUE)
corpus = tm_map(corpus,removeWords,lazy=TRUE)
corpus = tm_map(corpus,stemDocument,lazy=TRUE)
?DocumentTermMatrix
setwd("/Users/akshaykulkarni/Documents/Kaggle/Analytics_Edge_Summer_2015")
library(tm)
library(SnowballC)
eBayTrain = read.csv("eBayiPadTrain.csv", stringsAsFactors=FALSE)
eBayTest = read.csv("eBayiPadTest.csv", stringsAsFactors=FALSE)
corpus = Corpus(VectorSource(eBayTrain$description))
corpus = tm_map(corpus,tolower,lazy=TRUE)
corpus = tm_map(corpus,PlainTextDocument,lazy=TRUE)
corpus = tm_map(corpus,removePunctuation,lazy=TRUE)
corpus = tm_map(corpus,removeWords,lazy=TRUE)
corpus = tm_map(corpus,stemDocument,lazy=TRUE)
?tm_map
frequencies = DocumentTermMatrix(corpus)
sessionInfo()
setwd("/Users/akshaykulkarni/Documents/Kaggle/Analytics_Edge_Summer_2015")
library(tm)
library(SnowballC)
eBayTrain = read.csv("eBayiPadTrain.csv", stringsAsFactors=FALSE)
eBayTest = read.csv("eBayiPadTest.csv", stringsAsFactors=FALSE)
corpus = Corpus(VectorSource(eBayTrain$description))
corpus <- tm_map(corpus,
content_transformer(function(x) iconv(x, to='UTF-8-MAC', sub='byte')),
mc.cores=1)
corpus = tm_map(corpus,tolower,lazy=TRUE)
corpus = tm_map(corpus,PlainTextDocument,lazy=TRUE)
corpus = tm_map(corpus,removePunctuation,lazy=TRUE)
corpus = tm_map(corpus,removeWords,lazy=TRUE)
corpus = tm_map(corpus,stemDocument,lazy=TRUE)
frequencies = DocumentTermMatrix(corpus)
setwd("/Users/akshaykulkarni/Documents/Kaggle/Analytics_Edge_Summer_2015")
library(tm)
library(SnowballC)
eBayTrain = read.csv("eBayiPadTrain.csv", stringsAsFactors=FALSE)
eBayTest = read.csv("eBayiPadTest.csv", stringsAsFactors=FALSE)
corpus = Corpus(VectorSource(eBayTrain$description))
#corpus <- tm_map(corpus,content_transformer(function(x) iconv(x, to='UTF-8-MAC', sub='byte')),mc.cores=1)
corpus = tm_map(corpus,tolower,lazy=TRUE)
corpus = tm_map(corpus,PlainTextDocument,lazy=TRUE)
corpus = tm_map(corpus,removePunctuation,lazy=TRUE)
corpus = tm_map(corpus,removeWords,lazy=TRUE)
corpus = tm_map(corpus,stemDocument,lazy=TRUE)
frequencies = DocumentTermMatrix(corpus,mc.cores=1)
setwd("/Users/akshaykulkarni/Documents/Kaggle/Analytics_Edge_Summer_2015")
library(tm)
library(SnowballC)
eBayTrain = read.csv("eBayiPadTrain.csv", stringsAsFactors=FALSE)
eBayTest = read.csv("eBayiPadTest.csv", stringsAsFactors=FALSE)
corpus = Corpus(VectorSource(eBayTrain$description))
corpus = tm_map(corpus,content_transformer(function(x) iconv(x, to='UTF-8-MAC', sub='byte')),mc.cores=1)
corpus = tm_map(corpus,tolower,lazy=TRUE)
corpus = tm_map(corpus,PlainTextDocument,lazy=TRUE)
corpus = tm_map(corpus,removePunctuation,lazy=TRUE)
corpus = tm_map(corpus,removeWords,lazy=TRUE)
corpus = tm_map(corpus,stemDocument,lazy=TRUE)
frequencies = DocumentTermMatrix(corpus)
